{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f06f8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np \n",
    "import pylab \n",
    "import scipy.stats as stats \n",
    "from scipy.stats import boxcox\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "# from sklearn.preprocessing import Imputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.linear_model import Lasso\n",
    "from  sklearn.linear_model import Ridge\n",
    "from  sklearn.preprocessing  import PolynomialFeatures\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57e2eed",
   "metadata": {},
   "source": [
    "# Importing the data file and renaming the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2e2afda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(583249, 78)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the file\n",
    "df=pd.read_csv('Twitter.data')\n",
    "\n",
    "#renaming the columns\n",
    "data_cols=['NCD_0','NCD_1','NCD_2','NCD_3','NCD_4','NCD_5','NCD_6','AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6','AS(NA)_0','AS(NA)_1','AS(NA)_2','AS(NA)_3','AS(NA)_4','AS(NA)_5','AS(NA)_6','BL_0','BL_1','BL_2','BL_3','BL_4','BL_5','BL_6','NAC_0','NAC_1','NAC_2','NAC_3','NAC_4','NAC_5','NAC_6','AS(NAC)_0','AS(NAC)_1','AS(NAC)_2','AS(NAC)_3','AS(NAC)_4','AS(NAC)_5','AS(NAC)_6','CS_0','CS_1','CS_2','CS_3','CS_4','CS_5','CS_6','AT_0','AT_1','AT_2','AT_3','AT_4','AT_5','AT_6','NA_0','NA_1','NA_2','NA_3','NA_4','NA_5','NA_6',\n",
    "'ADL_0','ADL_1','ADL_2','ADL_3','ADL_4','ADL_5','ADL_6','NAD_0','NAD_1','NAD_2','NAD_3','NAD_4','NAD_5','NAD_6','Target']\n",
    "df.columns=data_cols\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1010f13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NCD_0</th>\n",
       "      <th>NCD_1</th>\n",
       "      <th>NCD_2</th>\n",
       "      <th>NCD_3</th>\n",
       "      <th>NCD_4</th>\n",
       "      <th>NCD_5</th>\n",
       "      <th>NCD_6</th>\n",
       "      <th>AI_0</th>\n",
       "      <th>AI_1</th>\n",
       "      <th>AI_2</th>\n",
       "      <th>...</th>\n",
       "      <th>ADL_5</th>\n",
       "      <th>ADL_6</th>\n",
       "      <th>NAD_0</th>\n",
       "      <th>NAD_1</th>\n",
       "      <th>NAD_2</th>\n",
       "      <th>NAD_3</th>\n",
       "      <th>NAD_4</th>\n",
       "      <th>NAD_5</th>\n",
       "      <th>NAD_6</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "      <td>583249.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>140.339881</td>\n",
       "      <td>136.770378</td>\n",
       "      <td>159.679545</td>\n",
       "      <td>181.592402</td>\n",
       "      <td>201.097788</td>\n",
       "      <td>220.175747</td>\n",
       "      <td>219.388589</td>\n",
       "      <td>71.038172</td>\n",
       "      <td>69.829749</td>\n",
       "      <td>82.198344</td>\n",
       "      <td>...</td>\n",
       "      <td>1.136689</td>\n",
       "      <td>1.140372</td>\n",
       "      <td>140.790102</td>\n",
       "      <td>137.181502</td>\n",
       "      <td>160.106196</td>\n",
       "      <td>182.057752</td>\n",
       "      <td>201.596826</td>\n",
       "      <td>220.706276</td>\n",
       "      <td>219.937239</td>\n",
       "      <td>191.279821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>431.772970</td>\n",
       "      <td>432.305464</td>\n",
       "      <td>502.057815</td>\n",
       "      <td>574.884157</td>\n",
       "      <td>630.448918</td>\n",
       "      <td>669.206442</td>\n",
       "      <td>672.182719</td>\n",
       "      <td>196.876865</td>\n",
       "      <td>202.199911</td>\n",
       "      <td>239.523223</td>\n",
       "      <td>...</td>\n",
       "      <td>1.432328</td>\n",
       "      <td>1.552315</td>\n",
       "      <td>432.625285</td>\n",
       "      <td>433.026946</td>\n",
       "      <td>502.774795</td>\n",
       "      <td>575.658466</td>\n",
       "      <td>631.258804</td>\n",
       "      <td>670.051490</td>\n",
       "      <td>673.033057</td>\n",
       "      <td>612.352828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>25.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>1.091298</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>101.000000</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>139.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>24210.000000</td>\n",
       "      <td>29574.000000</td>\n",
       "      <td>37505.000000</td>\n",
       "      <td>72366.000000</td>\n",
       "      <td>79079.000000</td>\n",
       "      <td>79079.000000</td>\n",
       "      <td>79079.000000</td>\n",
       "      <td>18654.000000</td>\n",
       "      <td>22035.000000</td>\n",
       "      <td>29402.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>295.000000</td>\n",
       "      <td>24301.000000</td>\n",
       "      <td>29574.000000</td>\n",
       "      <td>37505.000000</td>\n",
       "      <td>72366.000000</td>\n",
       "      <td>79083.000000</td>\n",
       "      <td>79083.000000</td>\n",
       "      <td>79083.000000</td>\n",
       "      <td>75724.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 78 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               NCD_0          NCD_1          NCD_2          NCD_3  \\\n",
       "count  583249.000000  583249.000000  583249.000000  583249.000000   \n",
       "mean      140.339881     136.770378     159.679545     181.592402   \n",
       "std       431.772970     432.305464     502.057815     574.884157   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       3.000000       4.000000       4.000000   \n",
       "50%        18.000000      17.000000      21.000000      24.000000   \n",
       "75%       104.000000     100.000000     115.000000     131.000000   \n",
       "max     24210.000000   29574.000000   37505.000000   72366.000000   \n",
       "\n",
       "               NCD_4          NCD_5          NCD_6           AI_0  \\\n",
       "count  583249.000000  583249.000000  583249.000000  583249.000000   \n",
       "mean      201.097788     220.175747     219.388589      71.038172   \n",
       "std       630.448918     669.206442     672.182719     196.876865   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         5.000000       6.000000       6.000000       2.000000   \n",
       "50%        27.000000      31.000000      30.000000      11.000000   \n",
       "75%       147.000000     166.000000     164.000000      59.000000   \n",
       "max     79079.000000   79079.000000   79079.000000   18654.000000   \n",
       "\n",
       "                AI_1           AI_2  ...          ADL_5          ADL_6  \\\n",
       "count  583249.000000  583249.000000  ...  583249.000000  583249.000000   \n",
       "mean       69.829749      82.198344  ...       1.136689       1.140372   \n",
       "std       202.199911     239.523223  ...       1.432328       1.552315   \n",
       "min         0.000000       0.000000  ...       0.000000       0.000000   \n",
       "25%         2.000000       2.000000  ...       1.000000       1.000000   \n",
       "50%        11.000000      13.000000  ...       1.000000       1.000000   \n",
       "75%        57.000000      65.000000  ...       1.090909       1.091298   \n",
       "max     22035.000000   29402.000000  ...     262.000000     295.000000   \n",
       "\n",
       "               NAD_0          NAD_1          NAD_2          NAD_3  \\\n",
       "count  583249.000000  583249.000000  583249.000000  583249.000000   \n",
       "mean      140.790102     137.181502     160.106196     182.057752   \n",
       "std       432.625285     433.026946     502.774795     575.658466   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         3.000000       3.000000       4.000000       4.000000   \n",
       "50%        18.000000      17.000000      21.000000      24.000000   \n",
       "75%       104.000000     101.000000     115.000000     131.000000   \n",
       "max     24301.000000   29574.000000   37505.000000   72366.000000   \n",
       "\n",
       "               NAD_4          NAD_5          NAD_6         Target  \n",
       "count  583249.000000  583249.000000  583249.000000  583249.000000  \n",
       "mean      201.596826     220.706276     219.937239     191.279821  \n",
       "std       631.258804     670.051490     673.033057     612.352828  \n",
       "min         0.000000       0.000000       0.000000       0.000000  \n",
       "25%         5.000000       6.000000       6.000000       4.500000  \n",
       "50%        27.000000      31.000000      31.000000      25.500000  \n",
       "75%       148.000000     167.000000     165.000000     139.000000  \n",
       "max     79083.000000   79083.000000   79083.000000   75724.500000  \n",
       "\n",
       "[8 rows x 78 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e656d248",
   "metadata": {},
   "source": [
    "**Data Leakage , the correlation between target and features are evaluated and Highly correlated values are chosen .**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb2d036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCD_0 0.8833155241537919\n",
      "NCD_1 0.8898837192714912\n",
      "NCD_2 0.8751676672058126\n",
      "NCD_3 0.8730600751939562\n",
      "NCD_4 0.8862617166142664\n",
      "NCD_5 0.9185645271710168\n",
      "NCD_6 0.9553295810047585\n",
      "AI_0 0.786590908437711\n",
      "AI_1 0.7860654107330474\n",
      "AI_2 0.7693692157364859\n",
      "AI_3 0.7565559780239234\n",
      "AI_4 0.7562184621430272\n",
      "AI_5 0.7868145502606632\n",
      "AI_6 0.8237510645977262\n",
      "AS(NA)_0 0.8433572488878484\n",
      "AS(NA)_1 0.8402291114465434\n",
      "AS(NA)_2 0.8192253038939571\n",
      "AS(NA)_3 0.8082579162614404\n",
      "AS(NA)_4 0.8036910029962802\n",
      "AS(NA)_5 0.8143806855421284\n",
      "AS(NA)_6 0.8355973683965561\n",
      "BL_0 0.0851463778230265\n",
      "BL_1 0.07745920914279071\n",
      "BL_2 0.09117389218473025\n",
      "BL_3 0.08503989801457194\n",
      "BL_4 0.07943795853784007\n",
      "BL_5 0.06362679828771851\n",
      "BL_6 0.06512651890807368\n",
      "NAC_0 0.8755331389321774\n",
      "NAC_1 0.8835890866697451\n",
      "NAC_2 0.869843224465814\n",
      "NAC_3 0.8693013537861048\n",
      "NAC_4 0.8833273279145414\n",
      "NAC_5 0.9151097552774543\n",
      "NAC_6 0.9518090460466618\n",
      "AS(NAC)_0 0.8539916009839821\n",
      "AS(NAC)_1 0.8504976847357768\n",
      "AS(NAC)_2 0.8377564623887398\n",
      "AS(NAC)_3 0.8340529012217986\n",
      "AS(NAC)_4 0.8334539618568949\n",
      "AS(NAC)_5 0.8450926301527578\n",
      "AS(NAC)_6 0.8663135348004404\n",
      "CS_0 0.08297351146239317\n",
      "CS_1 0.07566455667723561\n",
      "CS_2 0.08955929316772013\n",
      "CS_3 0.08307681398835526\n",
      "CS_4 0.07711313375633258\n",
      "CS_5 0.06051972114345019\n",
      "CS_6 0.061593443186307974\n",
      "AT_0 0.010956557342265558\n",
      "AT_1 0.009756305933422575\n",
      "AT_2 0.016897288039478115\n",
      "AT_3 0.011832420660447245\n",
      "AT_4 0.007521941008850326\n",
      "AT_5 -0.00039283030295094844\n",
      "AT_6 -0.0007864155484290902\n",
      "NA_0 0.8435211339259351\n",
      "NA_1 0.8508401056155269\n",
      "NA_2 0.8331935238362328\n",
      "NA_3 0.8290347551304061\n",
      "NA_4 0.8388342095588599\n",
      "NA_5 0.8684210494852631\n",
      "NA_6 0.9050612482405392\n",
      "ADL_0 0.006125457498802683\n",
      "ADL_1 0.005530765709938794\n",
      "ADL_2 0.010893174734649552\n",
      "ADL_3 0.00553038925715296\n",
      "ADL_4 0.0011113098805880784\n",
      "ADL_5 -0.007130799438630219\n",
      "ADL_6 -0.007004573364219063\n",
      "NAD_0 0.8830733091052839\n",
      "NAD_1 0.8897444635710331\n",
      "NAD_2 0.8750989518327736\n",
      "NAD_3 0.8730262187184649\n",
      "NAD_4 0.8862510562179047\n",
      "NAD_5 0.9185332748933782\n",
      "NAD_6 0.9552986480850981\n"
     ]
    }
   ],
   "source": [
    "#selecting the required columns\n",
    "columns = ['NCD_0','NCD_1','NCD_2','NCD_3','NCD_4','NCD_5','NCD_6','AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6','AS(NA)_0','AS(NA)_1','AS(NA)_2','AS(NA)_3','AS(NA)_4','AS(NA)_5','AS(NA)_6','BL_0','BL_1','BL_2','BL_3','BL_4','BL_5','BL_6','NAC_0','NAC_1','NAC_2','NAC_3','NAC_4','NAC_5','NAC_6','AS(NAC)_0','AS(NAC)_1','AS(NAC)_2','AS(NAC)_3','AS(NAC)_4','AS(NAC)_5','AS(NAC)_6','CS_0','CS_1','CS_2','CS_3','CS_4','CS_5','CS_6','AT_0','AT_1','AT_2','AT_3','AT_4','AT_5','AT_6','NA_0','NA_1','NA_2','NA_3','NA_4','NA_5','NA_6',\n",
    "'ADL_0','ADL_1','ADL_2','ADL_3','ADL_4','ADL_5','ADL_6','NAD_0','NAD_1','NAD_2','NAD_3','NAD_4','NAD_5','NAD_6']\n",
    "\n",
    "for name in columns:\n",
    "    print(name,np.corrcoef(df[name],df['Target'])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcc0be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AI_0',\n",
       " 'AI_1',\n",
       " 'AI_2',\n",
       " 'AI_3',\n",
       " 'AI_4',\n",
       " 'AI_5',\n",
       " 'AI_6',\n",
       " 'AS(NA)_0',\n",
       " 'AS(NA)_1',\n",
       " 'AS(NA)_2',\n",
       " 'AS(NA)_3',\n",
       " 'AS(NA)_4',\n",
       " 'AS(NA)_5',\n",
       " 'AS(NA)_6',\n",
       " 'NAC_0',\n",
       " 'NAC_1',\n",
       " 'NAC_2',\n",
       " 'NAC_3',\n",
       " 'NAC_4',\n",
       " 'NAC_5',\n",
       " 'NAC_6',\n",
       " 'AS(NAC)_0',\n",
       " 'AS(NAC)_1',\n",
       " 'AS(NAC)_2',\n",
       " 'AS(NAC)_3',\n",
       " 'AS(NAC)_4',\n",
       " 'AS(NAC)_5',\n",
       " 'AS(NAC)_6',\n",
       " 'CS_0',\n",
       " 'CS_1',\n",
       " 'CS_2',\n",
       " 'CS_3',\n",
       " 'CS_4',\n",
       " 'CS_5',\n",
       " 'CS_6',\n",
       " 'NA_0',\n",
       " 'NA_1',\n",
       " 'NA_2',\n",
       " 'NA_3',\n",
       " 'NA_4',\n",
       " 'NA_5',\n",
       " 'NA_6',\n",
       " 'NAD_0',\n",
       " 'NAD_1',\n",
       " 'NAD_2',\n",
       " 'NAD_3',\n",
       " 'NAD_4',\n",
       " 'NAD_5',\n",
       " 'NAD_6']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "['AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6','AS(NA)_0','AS(NA)_1','AS(NA)_2','AS(NA)_3','AS(NA)_4','AS(NA)_5','AS(NA)_6','NAC_0','NAC_1','NAC_2','NAC_3','NAC_4','NAC_5','NAC_6','AS(NAC)_0','AS(NAC)_1','AS(NAC)_2','AS(NAC)_3','AS(NAC)_4','AS(NAC)_5','AS(NAC)_6','CS_0','CS_1','CS_2','CS_3','CS_4','CS_5','CS_6','NA_0','NA_1','NA_2','NA_3','NA_4','NA_5','NA_6','NAD_0','NAD_1','NAD_2','NAD_3','NAD_4','NAD_5','NAD_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94246859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data frame with just the variables which are correlated to Target\n",
    "#extracting the features as X and target as Y \n",
    "X = df[['AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6','AS(NA)_0','AS(NA)_1','AS(NA)_2','AS(NA)_3','AS(NA)_4','AS(NA)_5','AS(NA)_6','NAC_0','NAC_1','NAC_2','NAC_3','NAC_4','NAC_5','NAC_6','AS(NAC)_0','AS(NAC)_1','AS(NAC)_2','AS(NAC)_3','AS(NAC)_4','AS(NAC)_5','AS(NAC)_6','CS_0','CS_1','CS_2','CS_3','CS_4','CS_5','CS_6','NA_0','NA_1','NA_2','NA_3','NA_4','NA_5','NA_6','NAD_0','NAD_1','NAD_2','NAD_3','NAD_4','NAD_5','NAD_6','Target']]\n",
    "y = X.pop('Target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "888e022f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AI_0</th>\n",
       "      <th>AI_1</th>\n",
       "      <th>AI_2</th>\n",
       "      <th>AI_3</th>\n",
       "      <th>AI_4</th>\n",
       "      <th>AI_5</th>\n",
       "      <th>AI_6</th>\n",
       "      <th>AS(NA)_0</th>\n",
       "      <th>AS(NA)_1</th>\n",
       "      <th>AS(NA)_2</th>\n",
       "      <th>...</th>\n",
       "      <th>NA_4</th>\n",
       "      <th>NA_5</th>\n",
       "      <th>NA_6</th>\n",
       "      <th>NAD_0</th>\n",
       "      <th>NAD_1</th>\n",
       "      <th>NAD_2</th>\n",
       "      <th>NAD_3</th>\n",
       "      <th>NAD_4</th>\n",
       "      <th>NAD_5</th>\n",
       "      <th>NAD_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AI_0  AI_1  AI_2  AI_3  AI_4  AI_5  AI_6  AS(NA)_0  AS(NA)_1  AS(NA)_2  \\\n",
       "0     2     1     0     0     0     0     4  0.000007  0.000003       0.0   \n",
       "1     1     0     0     0     0     4     1  0.000003  0.000000       0.0   \n",
       "2     1     0     0     1     0     0     1  0.000003  0.000000       0.0   \n",
       "3     0     1     0     0     1     2     3  0.000000  0.000003       0.0   \n",
       "4     1     0     0     1     2     3     0  0.000003  0.000000       0.0   \n",
       "\n",
       "   ...  NA_4  NA_5  NA_6  NAD_0  NAD_1  NAD_2  NAD_3  NAD_4  NAD_5  NAD_6  \n",
       "0  ...     0     0     4      2      1      0      0      0      0      4  \n",
       "1  ...     0     4     1      1      0      0      0      0      4      1  \n",
       "2  ...     0     0     1      1      0      0      1      0      0      1  \n",
       "3  ...     1     2     3      0      1      0      0      1      2      3  \n",
       "4  ...     2     3     0      1      0      0      1      2      3      0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "516545cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a class which can calculate VIF for each variable\n",
    "class ReduceVIF(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, thresh=5.0, impute=True, impute_strategy='median'):\n",
    "        # From looking at documentation, values between 5 and 10 are \"okay\".\n",
    "        # Above 10 is too high and so should be removed.\n",
    "        self.thresh = thresh\n",
    "        \n",
    "        # The statsmodel function will fail with NaN values, as such we have to impute them.\n",
    "        # By default we impute using the median value.\n",
    "        # This imputation could be taken out and added as part of an sklearn Pipeline.\n",
    "        if impute:\n",
    "            self.imputer = SimpleImputer(strategy=impute_strategy)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        print('ReduceVIF fit')\n",
    "        if hasattr(self, 'imputer'):\n",
    "            self.imputer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('ReduceVIF transform')\n",
    "        columns = X.columns.tolist()\n",
    "        if hasattr(self, 'imputer'):\n",
    "            X = pd.DataFrame(self.imputer.transform(X), columns=columns)\n",
    "        return ReduceVIF.calculate_vif(X, self.thresh)\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_vif(X, thresh=5.0):\n",
    "        # Taken from https://stats.stackexchange.com/a/253620/53565 and modified\n",
    "        dropped=True\n",
    "        while dropped:\n",
    "            variables = X.columns\n",
    "            dropped = False\n",
    "            vif = [variance_inflation_factor(X[variables].values, X.columns.get_loc(var)) for var in X.columns]\n",
    "            \n",
    "            max_vif = max(vif)\n",
    "            if max_vif > thresh:\n",
    "                maxloc = vif.index(max_vif)\n",
    "                print(f'Dropping {X.columns[maxloc]} with vif={max_vif}')\n",
    "                X = X.drop([X.columns.tolist()[maxloc]], axis=1)\n",
    "                dropped=True\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea1c96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = ReduceVIF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c06d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReduceVIF fit\n",
      "ReduceVIF transform\n",
      "Dropping NAC_5 with vif=1455.919162202067\n"
     ]
    }
   ],
   "source": [
    "#Analyzing which variables to drop - to remove multicollinearity\n",
    "X = transformer.fit_transform(X[X.columns[-49:]], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a5757",
   "metadata": {},
   "source": [
    "Finally decided to remove variables which are more than VIF 10 - Based on commonly used assumption and the below columns are chosen for further steps.\n",
    "\n",
    "To avoid overfitting and train regression models better with data values better these steps has been carried out. ['AI_0','AI_1','AI_2','AI_4','AI_5','AI_6','AS(NA)_3','AS(NAC)_0','AS(NAC)_2','CS_0','CS_1','NAD_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e844ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "['AI_0','AI_1','AI_2','AI_4','AI_5','AI_6','AS(NA)_3','AS(NAC)_0','AS(NAC)_2','CS_0','CS_1','NAD_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744ef330",
   "metadata": {},
   "source": [
    "Drawing distributions for each feature of AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e60cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols=['AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6']\n",
    "\n",
    "for name in data_cols:\n",
    "    print(name)\n",
    "    x = df[name]\n",
    "    plt.hist(x, bins=100)\n",
    "    plt.ylabel('No of times')\n",
    "    plt.show()\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9451dc",
   "metadata": {},
   "source": [
    "**Since the columns is not normally distrubuted, we are trying various combinations of log,sqrt and box cox to reduce skewness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320fcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#understanding the skewness of AI_0\n",
    "from scipy.stats import boxcox\n",
    "from scipy.stats import skew\n",
    "import math\n",
    "\n",
    "skness = skew(df['AI_0'])\n",
    "print(\"skness\",skness)\n",
    "\n",
    "x11=np.sqrt(df['AI_0'])\n",
    "skness_sqrt = skew(x11)\n",
    "print(\"skness_sqrt\",skness_sqrt)\n",
    "\n",
    "x12 = (x11+1).apply(np.log)\n",
    "skness_sqrt_log = skew(x12)\n",
    "print(\"skness_sqrt_log\",skness_sqrt_log) \n",
    "\n",
    "bxcx = boxcox(x12+1)[0] #---- we are selecting this transformation\n",
    "skness_sqrt_log_bxcx = skew(bxcx)\n",
    "print(\"skness_sqrt_log_bxcx\",skness_sqrt_log_bxcx)\n",
    "\n",
    "x = (df['AI_0']+1).apply(np.log)\n",
    "skness_log = skew(x11)\n",
    "print(\"skness_log\",skness_log)\n",
    "\n",
    "x11=np.sqrt(x)\n",
    "skness_log_sqrt = skew(x11)\n",
    "print(\"skness_log_sqrt\",skness_log_sqrt)\n",
    "\n",
    "bxcx = boxcox(x11+1)[0]\n",
    "skness_log_sqrt_bxcx = skew(bxcx)\n",
    "print(\"skness_log_sqrt_bxcx\",skness_log_sqrt_bxcx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2447eb0",
   "metadata": {},
   "source": [
    "**Now applying the same transformation techniques(sqrt, log and box cox) for other AI columns i.e. from AI_1 to AI_6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5132aa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cols_AI=['AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6']\n",
    "\n",
    "for name in data_cols_AI:\n",
    "    print(name)\n",
    "    from scipy.stats import boxcox\n",
    "    from scipy.stats import skew\n",
    "    import math\n",
    "\n",
    "    skness = skew(df[name])\n",
    "    print(\"skness\",skness)\n",
    "\n",
    "    x11=np.sqrt(df[name])\n",
    "    skness_sqrt = skew(x11)\n",
    "    print(\"skness_sqrt\",skness_sqrt)\n",
    "\n",
    "    x12 = (x11+1).apply(np.log)\n",
    "    skness_sqrt_log = skew(x12)\n",
    "    print(\"skness_sqrt_log\",skness_sqrt_log) #---- we are selecting this transformation\n",
    "    \n",
    "    bxcx = boxcox(x12+1)[0] #---- we are selecting this transformation\n",
    "    skness_sqrt_log_bxcx = skew(bxcx)\n",
    "    print(\"skness_sqrt_log_bxcx\",skness_sqrt_log_bxcx)   \n",
    "\n",
    "    #printing the q-q plot and histogram for the selected transformation - sqrt_log\n",
    "    #Checking the normality by drawing a Q-Q plot on transformed data \n",
    "    print(\"Q-Q Plot for the transformed column\")\n",
    "    stats.probplot(bxcx, dist=\"norm\", plot=pylab)\n",
    "    pylab.show()\n",
    "\n",
    "    #Checking the histogram of the untransformed data \n",
    "    print(\"histogram for the transformed data\")\n",
    "    plt.hist(bxcx, bins='auto')\n",
    "    plt.ylabel('No of times')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0eb28",
   "metadata": {},
   "source": [
    "**Drawing distributions for each variable of AS(NA)_3 ,AS(NAC)_0 ,AS(NAC)_2 ,CS_0 ,CS_1 ,NAD_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370f1496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import boxcox\n",
    "from scipy.stats import skew\n",
    "import math\n",
    "data_cols=['AS(NA)_3','AS(NAC)_0','AS(NAC)_2','CS_0','CS_1','NAD_3']\n",
    "#data_cols_AI=['AI_0','AI_1','AI_2','AI_3','AI_4','AI_5','AI_6']\n",
    "\n",
    "for name in data_cols:\n",
    "    print(name)\n",
    "    from scipy.stats import boxcox\n",
    "    from scipy.stats import skew\n",
    "    import math\n",
    "\n",
    "    skness = skew(df[name])\n",
    "    print(\"skness\",skness)\n",
    "\n",
    "    x11=np.sqrt(df[name])\n",
    "    skness_sqrt = skew(x11)\n",
    "    print(\"skness_sqrt\",skness_sqrt)\n",
    "\n",
    "    x12 = (x11+1).apply(np.log)\n",
    "    skness_sqrt_log = skew(x12)\n",
    "    print(\"skness_sqrt_log\",skness_sqrt_log) #---- we are selecting this transformation\n",
    "    \n",
    "    bxcx = boxcox(x12+1)[0] #---- we are selecting this transformation\n",
    "    skness_sqrt_log_bxcx = skew(bxcx)\n",
    "    print(\"skness_sqrt_log_bxcx\",skness_sqrt_log_bxcx)   \n",
    "\n",
    "    #printing the q-q plot and histogram for the selected transformation - sqrt_log\n",
    "    #Checking the normality by drawing a Q-Q plot on transformed data \n",
    "    print(\"Q-Q Plot for the transformed column\")\n",
    "    stats.probplot(bxcx, dist=\"norm\", plot=pylab)\n",
    "    pylab.show()\n",
    "\n",
    "    #Checking the histogram of the untransformed data \n",
    "    print(\"histogram for the transformed data\")\n",
    "    plt.hist(bxcx, bins='auto')\n",
    "    plt.ylabel('No of times')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058e4ab6",
   "metadata": {},
   "source": [
    "After determining the type of transformation , lets apply that transformed value in the data frame\n",
    "\n",
    "The sqrt-log transformation is applied to features : NCD_1,NCD_2,NCD_3,'AI_1','AI_2','AI_3'\n",
    "\n",
    "The square root , log transformation and boxcox transformations is applied to the following features: NCD_0,NCD_4,NCD_5,NCD_6,'AI_0','AI_4','AI_5','AI_6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a45b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_log_bxcx=['AI_0','AI_1','AI_5','AI_6','AS(NA)_3','AS(NAC)_0','AS(NAC)_2']\n",
    "sqrt_log =['AI_2','AI_3','AI_4','NAD_3']\n",
    "td=df\n",
    "for names1 in sqrt_log_bxcx: \n",
    "   #td[names1]=boxcox(np.sqrt(df[names1]).apply(np.log)+1)[0] \n",
    "    x=np.sqrt(df[names1])\n",
    "    x11 = (x+1).apply(np.log)\n",
    "    x12 = boxcox(x11+1)[0]\n",
    "    td[names1] = x12\n",
    "    \n",
    "for names2 in sqrt_log: \n",
    "    x2=np.sqrt(df[names2])\n",
    "    x21 = (x2+1).apply(np.log)\n",
    "    td[names2] = x21  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=td[['AI_2','AI_3','AI_4','NAD_3','AI_0','AI_1','AI_5','AI_6','AS(NA)_3','AS(NAC)_0','AS(NAC)_2']]\n",
    "y=td['Target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eada24",
   "metadata": {},
   "source": [
    "Performing the testing Train split for all the data values for the transformed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99a8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train,y, random_state = 0,test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcfc534",
   "metadata": {},
   "source": [
    "After preforming scaling of data and running the models , it was observed that there was not much of impact in results due to scaling and the code has been commented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1645c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#X_train = scaler.fit_transform(X_train_unscaled)\n",
    "#X_test = scaler.transform(X_text_unscaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fa328",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg = LinearRegression()\n",
    "a=lreg.fit(X_train, y_train)\n",
    "print('training_score',lreg.score(X_train, y_train))\n",
    "print('test_score',lreg.score(X_test, y_test))\n",
    "#a.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e942a830",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b744d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "for n in range(1,4):\n",
    "    poly = PolynomialFeatures(n)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    lreg.fit(X_train_poly, y_train)\n",
    "    train_score_list.append(lreg.score(X_train_poly, y_train))\n",
    "    test_score_list.append(lreg.score(X_test_poly, y_test))\n",
    "print(train_score_list)\n",
    "print(test_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf47805",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "x_axis = range(1,4)\n",
    "plt.plot(x_axis, train_score_list, c = 'g', label = 'Train Score')\n",
    "plt.plot(x_axis, test_score_list, c = 'b', label = 'Test Score')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10734fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trial for alpha values for polynomial ridge\n",
    "x_range = [0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "train_score_list = []\n",
    "test_score_list = []\n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "for alpha_1 in x_range: \n",
    "    ridge = Ridge(alpha_1)\n",
    "    ridge.fit(X_train_poly,y_train)\n",
    "    train_score_list.append(ridge.score(X_train_poly,y_train))\n",
    "    test_score_list.append(ridge.score(X_test_poly, y_test))\n",
    "       \n",
    "    print(train_score_list)\n",
    "    print(test_score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d6ba14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Ridge regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0, 1, 10]:\n",
    "    poly = PolynomialFeatures(3)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    linridge = Ridge(alpha = this_alpha).fit(X_train_poly, y_train)\n",
    "    r2_train = linridge.score(X_train_poly, y_train)\n",
    "    r2_test = linridge.score(X_test_poly, y_test)\n",
    "    print(\"train_score  poly ridge\",  r2_train)\n",
    "    print(\"test_score  poly ridge\",r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877987cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best alpha by gridsearch\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'alpha': [ 0.001,0.01,0.1,0,1]}\n",
    "grid_search = GridSearchCV(Ridge(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_poly, y_train)\n",
    "\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\n",
    "\n",
    "grid_search= GridSearchCV(ridge(),param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568d9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Polynomial ridge degree 2 with alpha 0.001 as best param \n",
    "\n",
    "poly = PolynomialFeatures(3)\n",
    "X_train_poly = poly.fit_transform(X_train)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "linridge = Ridge(alpha =0).fit(X_train_poly, y_train)\n",
    "r2_train = linridge.score(X_train_poly, y_train)\n",
    "r2_test = linridge.score(X_test_poly, y_test)\n",
    "print(\"train_score  poly ridge \",  r2_train)\n",
    "print(\"test_score  poly ridge\",r2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1291d059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lasso regression: effect of alpha regularization parameter\\n')\n",
    "for this_alpha in [0.1,0,1]:\n",
    "    linlasso = Lasso(alpha = this_alpha).fit(X_train_poly, y_train)\n",
    "    r2_train = linlasso.score(X_train_poly, y_train)\n",
    "    r2_test = linlasso.score(X_test_poly, y_test)\n",
    "    num_coeff_bigger = np.sum(abs(linlasso.coef_) > 1.0)\n",
    "    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\n",
    "    r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n",
    "    .format(this_alpha, num_coeff_bigger, r2_train, r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac82df",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1c883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "train_score_array = []\n",
    "test_score_array = []\n",
    "\n",
    "for k in range(1,10):\n",
    "    knn_reg = KNeighborsRegressor(k)\n",
    "    knn_reg.fit(X_train, y_train)\n",
    "    train_score_array.append(knn_reg.score(X_train, y_train))\n",
    "    test_score_array.append(knn_reg.score(X_test, y_test))\n",
    "\n",
    "x_axis = range(1,10)\n",
    "plt.plot(x_axis, train_score_array, c = 'g', label = 'Train Score')\n",
    "plt.plot(x_axis, test_score_array, c = 'b', label = 'Test Score')\n",
    "plt.legend()\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2a31ec",
   "metadata": {},
   "source": [
    "**N neighbors with 2 value is best param in knn regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c538eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "knnreg = KNeighborsRegressor(n_neighbors = 2).fit(X_train, y_train) print('R-squared test score: \n",
    "{:.3f}'.format(knnreg.score(X_test, y_test))) print('R-squared train score: {:.3f}'.format(knnreg.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97e1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE for train data set :', metrics.mean_absolute_error(y_train, linridge.predict(X_train)))\n",
    "x_pred=knnreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bed557",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred=knnreg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3cc725",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#x_axis = range(1,4)\n",
    "plt.scatter(x_pred, y_test)\n",
    "#plt.scatter(y_test, c = 'b')\n",
    "plt.xlabel('Predicted values of number of active discussions' )\n",
    "plt.ylabel('actual  Buzz number of active discussions')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d9044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating SVR model with C=1000 and gamma=1\n",
    "svr = SVR( epsilon = 0.01,kernel='linear', C=1, gamma=100)\n",
    "#svr_rbf = SVR( epsilon = 0.01,kernel='rbf', C=1000, gamma=0.001)\n",
    "\n",
    "#calculating score and RME\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "print('R-squared  score(train) : {:.3f}'\n",
    "     .format(svr.score(X_train,y_train)))\n",
    "print('R-squared  score(test) : {:.3f}'\n",
    "     .format(svr.score(X_test,y_test)))\n",
    "print('MAE for train data set :', metrics.mean_absolute_error(y_train, svr.predict(X_train)))\n",
    "svr_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('R-squared  score(train) : {:.3f}'\n",
    "     .format(svr_rbf.score(X_train_scaled,y_train)))\n",
    "print('R-squared  score(test) : {:.3f}'\n",
    "     .format(svr_rbf.score(X_test,y_test)))\n",
    "print('MAE for train data set :', metrics.mean_absolute_error(y_train, svr_rbf.predict(X_train_scaled)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ac2b75",
   "metadata": {},
   "source": [
    "The models where executed and the best model is polynomial ridge with R square value for training is 0.8424 and R score for testing score is 0.80 and The best parameter for alpha is 0 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a68cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
